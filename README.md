# ğŸ§  Mistral 7B Web Chatbot

An interactive **AI-powered chatbot web application** built using **Mistral 7B**, an open-source large language model by [Mistral AI](https://mistral.ai).  
This project allows you to run and chat with a powerful LLM **locally on your own system**, using **Ollama** as the model runtime and a modern **React + Express.js** web interface.

---

## ğŸš€ Features

- âš¡ **Locally hosted AI** using [Ollama](https://ollama.com) â€” no API keys or internet required  
- ğŸ’¬ **Interactive chat interface** built with **React.js**  
- ğŸ”„ **Express.js backend API** that communicates with the Mistral model  
- ğŸŒ **REST-based architecture** for easy integration and scaling  
- ğŸ”’ **Cross-origin support (CORS)** for seamless frontend-backend communication  
- ğŸ§© **Lightweight and extendable** for future LLM integrations or cloud deployment  

---

## ğŸ§± Tech Stack

| Layer | Technology |
|--------|-------------|
| Frontend | React.js, HTML, CSS |
| Backend | Node.js, Express.js |
| AI Engine | Mistral 7B via Ollama |
| Language | JavaScript (ES6) |
| Protocol | REST API |

---

## âš™ï¸ System Architecture

```text
[User Interface (React)]
          â†“
[Express.js Backend API]
          â†“
[Ollama Local Server]
          â†“
[Mistral 7B Model Inference Engine]
```

## ğŸ§± Tech Stack

| Layer | Technology |
|--------|-------------|
| Frontend | React.js, HTML, CSS |
| Backend | Node.js, Express.js |
| AI Engine | Mistral 7B via Ollama |
| Language | JavaScript (ES6) |
| Protocol | REST API |

---

## âš™ï¸ System Architecture

```text
[User Interface (React)]
          â†“
[Express.js Backend API]
          â†“
[Ollama Local Server]
          â†“
[Mistral 7B Model Inference Engine]

```
ğŸ§© Prerequisites

Node.js
(v18 or later)
npm
Ollama

ğŸ§° Setup Guide
1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/mistral-7b-chatbot.git
cd mistral-7b-chatbot

2ï¸âƒ£ Install and Run Ollama

Download Ollama â†’ https://ollama.com/download

Pull the Mistral model

ollama pull mistral


This downloads the Mistral 7B model (~4.4 GB) to your local system.

Start the Ollama API server

ollama serve


It will now run at:

http://localhost:11434

3ï¸âƒ£ Setup and Run the Backend (Express Server)

Initialize a Node project (creates a package.json file)

npm init -y


Install required dependencies

npm install express body-parser cors node-fetch


Make sure your package.json contains this (for ES modules):

{
  "name": "mistral-7b-chatbot",
  "version": "1.0.0",
  "type": "module",
  "main": "server.js",
  "dependencies": {
    "express": "^4.19.0",
    "body-parser": "^1.20.3",
    "cors": "^2.8.5",
    "node-fetch": "^3.3.2"
  }
}


Start your backend server

node server.js


You should see:

Server running on http://localhost:5000

4ï¸âƒ£ Test the Backend API (Optional)
ğŸªŸ PowerShell:
Invoke-WebRequest -Uri "http://localhost:5000/chat" -Method POST -Headers @{ "Content-Type" = "application/json" } -Body '{"prompt":"Hello, Mistral!"}'

ğŸ’» Git Bash / CMD:
curl -X POST http://localhost:5000/chat -H "Content-Type: application/json" -d "{\"prompt\": \"Hello, Mistral!\"}"


âœ… You should receive a JSON response containing the modelâ€™s reply.

5ï¸âƒ£ (Optional) Setup React Frontend

If you have a client/ folder containing the React project:

cd client
npm install
npm start


This will start the web UI at:

http://localhost:3000

ğŸ’¬ Example Workflow

Start Ollama â†’ ollama serve

Start Backend â†’ node server.js

Start Frontend â†’ npm start

Open browser â†’ http://localhost:3000

Start chatting with Mistral 7B ğŸš€

ğŸ”® Future Enhancements

ğŸ§  Add conversation memory (context persistence)

ğŸ—‚ï¸ Integrate vector databases for knowledge-based Q&A

ğŸ¤ Voice input/output (Speech-to-Text & TTS)

âš¡ Streaming responses for real-time output

â˜ï¸ Cloud deployment using GPU-backed services (RunPod, Paperspace, etc.)

ğŸ§‘â€ğŸ’» Author

Your Name â€” Full Stack / AI Developer
ğŸ”— GitHub: @yourusername

ğŸ’¬ â€œBuilt with open-source intelligence for open-source innovation.â€

ğŸ“œ License

This project is licensed under the MIT License â€” youâ€™re free to use, modify, and distribute it with attribution.
